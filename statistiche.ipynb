{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1993a206-4b1e-4561-9c5b-1b626fc715e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/guest/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Assicurati di avere i tokenizer di NLTK\n",
    "nltk.download('punkt')\n",
    "\n",
    "def compute_statistics(file_path, column_name):\n",
    "    # Carica il file CSV con gestione degli errori di parsing\n",
    "    df = pd.read_csv(file_path, sep=\";\", on_bad_lines='skip', dtype={column_name: str})\n",
    "    \n",
    "    # Sostituisci NaN e valori non stringa con stringhe vuote\n",
    "    df[column_name] = df[column_name].fillna(\"\").astype(str)\n",
    "    \n",
    "    # Filtra eventuali righe completamente vuote\n",
    "    df = df[df[column_name].str.strip() != \"\"]\n",
    "\n",
    "    # Conta il numero di data points\n",
    "    num_data_points = len(df)\n",
    "\n",
    "    # Tokenizzazione e conteggi\n",
    "    tokenized_texts = [word_tokenize(text) for text in df[column_name]]\n",
    "    num_tokens = sum(len(tokens) for tokens in tokenized_texts)\n",
    "    avg_tokens_per_caption = num_tokens / num_data_points if num_data_points > 0 else 0\n",
    "    unique_words = set(word for tokens in tokenized_texts for word in tokens)\n",
    "\n",
    "    # Stampa le statistiche\n",
    "    print(f\"Statistics for {file_path} ({column_name} column):\")\n",
    "    print(f\"Number of data points: {num_data_points}\")\n",
    "    print(f\"Total number of tokens: {num_tokens}\")\n",
    "    print(f\"Average number of tokens per caption: {avg_tokens_per_caption:.2f}\")\n",
    "    print(f\"Number of unique words: {len(unique_words)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# File paths (modifica con il percorso corretto)\n",
    "goldcorpus_path = \"goldcorpus_comparisons.csv\"\n",
    "europarl_path = \"europarl_unito_500righe.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74256b5b-842a-4809-9295-ed2ee9b3f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_statistics(goldcorpus_path, \"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1322fcf6-e2e8-40b7-8b3a-fb4959db9132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for europarl_unito_500righe.csv (Inglese column):\n",
      "Number of data points: 499\n",
      "Total number of tokens: 15356\n",
      "Average number of tokens per caption: 30.77\n",
      "Number of unique words: 2481\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_statistics(europarl_path, \"Inglese\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dfff49b-be2b-48ca-9559-dbca166a2e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/guest/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a9680e682a4b52bc276bb497465638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for laion/laion2B-en (caption column, rows 7000-8000):\n",
      "Number of data points: 1000\n",
      "Total number of tokens: 11885\n",
      "Average number of tokens per caption: 11.88\n",
      "Number of unique words: 5999\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Assicurati di avere i tokenizer di NLTK\n",
    "nltk.download('punkt')\n",
    "\n",
    "def compute_statistics_from_dataset(dataset_name, split, column_name, start, end):\n",
    "    # Carica solo le righe necessarie\n",
    "    dataset = load_dataset(dataset_name, split=split, streaming=True)\n",
    "    \n",
    "    # Seleziona solo le righe necessarie\n",
    "    texts = []\n",
    "    for i, row in enumerate(dataset):\n",
    "        if i >= start and i < end:\n",
    "            texts.append(row[column_name])\n",
    "        if i >= end:\n",
    "            break\n",
    "    \n",
    "    # Conta il numero di data points\n",
    "    num_data_points = len(texts)\n",
    "    \n",
    "    # Tokenizzazione e conteggi\n",
    "    tokenized_texts = [word_tokenize(text) for text in texts]\n",
    "    num_tokens = sum(len(tokens) for tokens in tokenized_texts)\n",
    "    avg_tokens_per_caption = num_tokens / num_data_points if num_data_points > 0 else 0\n",
    "    unique_words = set(word for tokens in tokenized_texts for word in tokens)\n",
    "    \n",
    "    # Stampa le statistiche\n",
    "    print(f\"Statistics for {dataset_name} ({column_name} column, rows {start}-{end}):\")\n",
    "    print(f\"Number of data points: {num_data_points}\")\n",
    "    print(f\"Total number of tokens: {num_tokens}\")\n",
    "    print(f\"Average number of tokens per caption: {avg_tokens_per_caption:.2f}\")\n",
    "    print(f\"Number of unique words: {len(unique_words)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Calcola statistiche per il dataset LAION-2B-EN-RESEARCHSAFE\n",
    "compute_statistics_from_dataset(\"laion/laion2B-en\", \"train\", \"caption\", 7000, 8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f1c72-ff05-4170-82dc-0ebdb4a61fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-calderini",
   "language": "python",
   "name": "venv-calderini"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
